{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c8c601",
   "metadata": {},
   "source": [
    "# Melbourne Housing Prices - Missing Values\n",
    "Through out this Kaggle course for intermediate machine learning level, I learned three different approaches to dealing with missing values and compared effectiveness these approaches on a real-world dataset.\n",
    " 1) A Simple Option: Drop Columns with Missing Values\n",
    " 2) A Better Option: Imputation\n",
    " 3) An Extension To Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b27d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# Select target\n",
    "y = data.Price\n",
    "\n",
    "# To keep things simple, use only numerical predictors\n",
    "melb_predictors = data.drop(['Price'], axis=1)\n",
    "X = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b07e5",
   "metadata": {},
   "source": [
    "### code explanation above\n",
    "- melb_predictors = data.drop(['Price'], axis=1) : \n",
    "The 'Price' column is typically the target variable in a machine learning task, and you're removing it because you want to use the remaining columns as predictor variables. The axis=1 argument indicates that you're dropping a column (as opposed to a row), and the result is a DataFrame without the 'Price' column.\n",
    "- X = melb_predictors.select_dtypes(exclude=['object']) : \n",
    "excluding columns with the data type 'object' (commonly used for text or categorical data) and creating a new DataFrame called X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a462b",
   "metadata": {},
   "source": [
    "### Define Function to Measure Quality of Each Approach\n",
    "Define a function \"score_dataset()\" to compare different approaches to dealing with missing values. This function reports the mean absolute error (MAE) from a random forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a9a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171296c",
   "metadata": {},
   "source": [
    "#### Explained about n_estimators=10 above: \n",
    "In the code you provided, the n_estimators parameter for the RandomForestRegressor is set to 10. The n_estimators parameter specifies the number of decision trees in the random forest ensemble. In this case, the value of 10 means that the random forest will consist of 10 decision trees.\n",
    "\n",
    "The choice of n_estimators is a hyperparameter, and it determines how many trees are used in the ensemble. The number of trees can have an impact on the model's performance. Typically, increasing the number of trees can make the model more robust and accurate, but it also increases the computational complexity and training time.\n",
    "\n",
    "A value of 10 for n_estimators is relatively low, and it may be used for quick experimentation and initial model testing. It can provide a baseline for model performance without requiring a significant amount of computational resources. However, for more accurate and robust results, it's common to use a larger number of trees, often in the hundreds or even thousands, depending on the dataset and problem.\n",
    "\n",
    "The choice of the best n_estimators value depends on the specific problem and dataset. It is common practice to perform hyperparameter tuning, such as using techniques like cross-validation, to find the optimal number of trees for a given problem. This allows you to strike a balance between model accuracy and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2573ad",
   "metadata": {},
   "source": [
    "## Score from Approach 1 (Drop Columns with Missing Values)\n",
    "since I am working with both training and validation sets, I am careful to drop the same columns in both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17bfe81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop columns with missing values):\n",
      "183550.22137772635\n"
     ]
    }
   ],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                    if X_train[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccbebda",
   "metadata": {},
   "source": [
    "## Score from Approach 2 (Imputation)\n",
    "Next, use \"SimpleImputer\" to replace missing values withthe mean value along each column.\n",
    "Although it's simple, filing in the mean value generally performs quite well (but this varies by dataset). While statisticians have experimented with more complex ways to determine imputed values (such as regression imputation, for instance), the complex strategies typically give no additional benefit once you plug the results into sophisticated machine learning models.\n",
    "*Imputation: process of filling in or estimating missing values in a dataset using various methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db27a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Imputation):\n",
      "178166.46269899711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print(\"MAE from Approach 2 (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5456e",
   "metadata": {},
   "source": [
    "#### Explanation about putting back imputation removed column names\n",
    "- Imputation with SimpleImputer: When you use SimpleImputer to fill in missing values in your data, it essentially creates a NumPy array without column names because SimpleImputer works with arrays. The resulting imputed_X_train and imputed_X_valid do not have column names attached to them.\n",
    "\n",
    "- Putting Back Column Names: Column names are essential for maintaining the structure and interpretability of your dataset. Without column names, it can be challenging to understand the meaning of each feature or variable in your data. Additionally, if you plan to use these imputed datasets for training machine learning models or for any further analysis, it's important that the columns are correctly labeled so that the models can work with the features as intended.\n",
    "\n",
    "By assigning the column names back to the imputed DataFrames using imputed_X_train.columns = X_train.columns and imputed_X_valid.columns = X_valid.columns, you are essentially ensuring that the imputed data retains the original structure and feature names. This step is crucial to maintain data consistency and readability and to avoid potential errors or confusion when working with the imputed datasets in subsequent steps of your data analysis or modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ef37a",
   "metadata": {},
   "source": [
    "## Score from Approach 3 (An Extension to Imputation)\n",
    "Impute the missing values, while also keeping track of which values were imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9265b757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (An Extention to Imputation):\n",
      "178927.503183954\n"
     ]
    }
   ],
   "source": [
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "    \n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE from Approach 3 (An Extention to Imputation):\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90cc17",
   "metadata": {},
   "source": [
    "So, why did imputation perform better than dropping the columns?\n",
    "The training data has 10864 rows and 12 columns, where three columns contain missing data. For each column, less than half of the entries are missing. Thus, dropping the columns removes a lot of useful information, and so it makes sense that imputation would perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d47c8442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10864, 12)\n",
      "Car               49\n",
      "BuildingArea    5156\n",
      "YearBuilt       4307\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shape of training data (num_rows, num_columns)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Number of missing values in each columns of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f42ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
